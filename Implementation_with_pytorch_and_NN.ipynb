{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM67XX3wnoBjw/LkpvYeL+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SakshiKasture/MNIST_digit_classification/blob/main/Implementation_with_pytorch_and_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qaUIEb2NblOM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the Dataset**"
      ],
      "metadata": {
        "id": "YkNWPVGOe9y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a transformation\n",
        "'''transforms.Compose() is used to chain multiple image transformations together.\n",
        "This transformation converts the image from a PIL (Python Imaging Library) or NumPy array format to a PyTorch tensor.\n",
        "Additionally, it normalizes the pixel values of the image to a range of [0, 1] by dividing by 255.\n",
        "This step is essential because neural networks typically work with tensors, not images.\n",
        "It converts an image of shape (H, W, C) (Height, Width, Channels) into a tensor of shape (C, H, W).\n",
        "transforms.Normalize(mean, std)\n",
        "This transformation normalizes the image tensor by subtracting the mean and dividing by the standard deviation.\n",
        "It makes the model training more stable by ensuring the data has a standard distribution (zero mean, unit variance).\n",
        "The mean and std values depend on the dataset. For MNIST, the mean and std are often chosen as (0.5,) to bring pixel values into a roughly centered range.\n",
        "After normalization, the tensor will have a mean of 0 and a standard deviation of 1.\n",
        "Normalize works on tensors, not on images. If you try to apply Normalize directly to an image, it will raise an error because\n",
        "Normalize expects a tensor as input, and the image is still in its original format (e.g., PIL or NumPy).\n",
        "The image needs to be converted to a tensor first using ToTensor(), and then Normalize() can operate on the tensor.'''\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
        ")\n",
        "\n",
        "#Load the datasets and test datasets\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "#Creation of Dataloaders\n",
        "dataloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "dDcTS91-ciP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e82eb0-45fd-4b4f-dc57-8147d4421e24"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 351kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.05MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the Neural Network**"
      ],
      "metadata": {
        "id": "8M8CKu3Ifh9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Input Layer (28 * 28): Matches the flattened size of the MNIST input image (28x28 pixels).\n",
        "'''Hidden Layers (128, 64):\n",
        "\n",
        "These are chosen based on how complex the task is and how much information needs to be learned.\n",
        "Larger values like 128 allow the network to learn more features but increase computational cost.\n",
        "Smaller values like 64 reduce computation but might limit the model's capacity.\n",
        "Common practice: Start with a reasonable size (e.g., 128) and decrease gradually (64) as you go deeper.\n",
        "In summary, the sizes (128, 64, etc.) are hyperparameters and are chosen based on experimentation, considering the task's complexity and computational efficiency.'''\n",
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleNN, self).__init__()\n",
        "     # Input layer: 28x28 images (flattened to 784 features)\n",
        "    self.fc1 = nn.Linear(28*28, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 10) # Output layer (10 classes for MNIST digits)\n",
        "\n",
        "  def forward(self,x):\n",
        "     x = x.view(x.size(0), -1)\n",
        "      #ReLU (Rectified Linear Unit) is applied to introduce non-linearity, allowing the model to learn complex patterns in the data.\n",
        "     x = torch.relu(self.fc1(x)) # Apply ReLU activation\n",
        "     x = torch.relu(self.fc2(x)) # Apply ReLU activation\n",
        "     x = self.fc3(x) # Output layer\n",
        "     return x"
      ],
      "metadata": {
        "id": "vPu_aVoefgB1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup of Loss function and Optimizer**"
      ],
      "metadata": {
        "id": "2sSXSzhDhxrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9aSWd4hKhxLt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**"
      ],
      "metadata": {
        "id": "PhM2L-BLiUeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "'''This loop runs the training process for a specified number of epochs (iterations over the entire dataset).\n",
        "Each epoch means the model sees all the training data once.running_loss = 0.0\n",
        "This variable tracks the cumulative loss within a single epoch, helping to monitor how well the model is learning.\n",
        "Here, trainloader (a PyTorch DataLoader) provides batches of training data. Each batch contains:\n",
        "inputs: Features (e.g., images).\n",
        "labels: Corresponding targets (e.g., class labels).\n",
        "enumerate gives both the batch index i and the data (inputs, labels).\n",
        "Imp step - optimizer.zero_grad()\n",
        "Gradients from the previous batch are cleared. If this step is skipped, gradients will accumulate, leading to incorrect updates during backpropagation.\n",
        "outputs = model(inputs)\n",
        "The inputs are passed through the model, which computes predictions (outputs). This is called the forward pass and is the starting point for calculating loss.\n",
        "'''\n",
        "epochs = 5 # Number of epochs to train\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  for i, (inputs, labels) in enumerate(dataloader,0):\n",
        "    optimizer.zero_grad() # Zero the parameter gradients\n",
        "  # Forward pass\n",
        "    outputs = model(inputs)\n",
        "     # Calculate loss\n",
        "     #The labels are typically obtained from the DataLoader during the training loop, where each batch provides both inputs (features) and labels (targets).\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Backward pass and optimization\n",
        "    #Computes the gradients of the loss with respect to all the model's parameters (weights and biases).\n",
        "    #The computed gradients are essential for updating the model's parameters during optimization.\n",
        "    loss.backward()\n",
        "    #The optimizer (e.g., Adam, SGD) adjusts the parameters in the direction that minimizes the loss.\n",
        "    #This is the step where learning happens.\n",
        "    optimizer.step()\n",
        "    #Adds the current batch's loss to running_loss.\n",
        "    #loss.item() extracts the scalar value of the loss (as PyTorch's loss tensor holds more than just the number).\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i % 100 == 99:\n",
        "      '''Every 100 batches (i % 100 == 99), print the average loss for those batches.\n",
        "          The loss is averaged by dividing running_loss by 100.\n",
        "          Helps monitor whether the loss is decreasing over time, indicating effective learning.\n",
        "          After printing, running_loss is reset to 0.0 for the next 100 batches.\n",
        "          '''\n",
        "      print(f\"Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100:.4f}\")\n",
        "      running_loss = 0.0\n",
        "print(\"Finished training!\")"
      ],
      "metadata": {
        "id": "mzpfKVQ4iXFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff0b540-a54b-415a-88ec-afac952e32e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.3167\n",
            "Epoch 1, Batch 200, Loss: 0.5599\n",
            "Epoch 1, Batch 300, Loss: 0.4627\n",
            "Epoch 1, Batch 400, Loss: 0.4177\n",
            "Epoch 1, Batch 500, Loss: 0.3684\n",
            "Epoch 1, Batch 600, Loss: 0.3582\n",
            "Epoch 1, Batch 700, Loss: 0.3168\n",
            "Epoch 1, Batch 800, Loss: 0.3316\n",
            "Epoch 1, Batch 900, Loss: 0.2984\n",
            "Epoch 1, Batch 1000, Loss: 0.2784\n",
            "Epoch 1, Batch 1100, Loss: 0.2774\n",
            "Epoch 1, Batch 1200, Loss: 0.2764\n",
            "Epoch 1, Batch 1300, Loss: 0.2632\n",
            "Epoch 1, Batch 1400, Loss: 0.2737\n",
            "Epoch 1, Batch 1500, Loss: 0.2562\n",
            "Epoch 1, Batch 1600, Loss: 0.2317\n",
            "Epoch 1, Batch 1700, Loss: 0.2391\n",
            "Epoch 1, Batch 1800, Loss: 0.2148\n",
            "Epoch 2, Batch 100, Loss: 0.2147\n",
            "Epoch 2, Batch 200, Loss: 0.2040\n",
            "Epoch 2, Batch 300, Loss: 0.1907\n",
            "Epoch 2, Batch 400, Loss: 0.2034\n",
            "Epoch 2, Batch 500, Loss: 0.2110\n",
            "Epoch 2, Batch 600, Loss: 0.1993\n",
            "Epoch 2, Batch 700, Loss: 0.1696\n",
            "Epoch 2, Batch 800, Loss: 0.1754\n",
            "Epoch 2, Batch 900, Loss: 0.1919\n",
            "Epoch 2, Batch 1000, Loss: 0.1539\n",
            "Epoch 2, Batch 1100, Loss: 0.1808\n",
            "Epoch 2, Batch 1200, Loss: 0.1472\n",
            "Epoch 2, Batch 1300, Loss: 0.1773\n",
            "Epoch 2, Batch 1400, Loss: 0.1587\n",
            "Epoch 2, Batch 1500, Loss: 0.1694\n",
            "Epoch 2, Batch 1600, Loss: 0.1636\n",
            "Epoch 2, Batch 1700, Loss: 0.1559\n",
            "Epoch 2, Batch 1800, Loss: 0.1343\n",
            "Epoch 3, Batch 100, Loss: 0.1314\n",
            "Epoch 3, Batch 200, Loss: 0.1239\n",
            "Epoch 3, Batch 300, Loss: 0.1222\n",
            "Epoch 3, Batch 400, Loss: 0.1567\n",
            "Epoch 3, Batch 500, Loss: 0.1321\n",
            "Epoch 3, Batch 600, Loss: 0.1145\n",
            "Epoch 3, Batch 700, Loss: 0.1307\n",
            "Epoch 3, Batch 800, Loss: 0.1502\n",
            "Epoch 3, Batch 900, Loss: 0.1366\n",
            "Epoch 3, Batch 1000, Loss: 0.1355\n",
            "Epoch 3, Batch 1100, Loss: 0.1530\n",
            "Epoch 3, Batch 1200, Loss: 0.1335\n",
            "Epoch 3, Batch 1300, Loss: 0.1393\n",
            "Epoch 3, Batch 1400, Loss: 0.1299\n",
            "Epoch 3, Batch 1500, Loss: 0.1230\n",
            "Epoch 3, Batch 1600, Loss: 0.1194\n",
            "Epoch 3, Batch 1700, Loss: 0.1162\n",
            "Epoch 3, Batch 1800, Loss: 0.1302\n",
            "Epoch 4, Batch 100, Loss: 0.0834\n",
            "Epoch 4, Batch 200, Loss: 0.1026\n",
            "Epoch 4, Batch 300, Loss: 0.1232\n",
            "Epoch 4, Batch 400, Loss: 0.1293\n",
            "Epoch 4, Batch 500, Loss: 0.0893\n",
            "Epoch 4, Batch 600, Loss: 0.1144\n",
            "Epoch 4, Batch 700, Loss: 0.1225\n",
            "Epoch 4, Batch 800, Loss: 0.1255\n",
            "Epoch 4, Batch 900, Loss: 0.1055\n",
            "Epoch 4, Batch 1000, Loss: 0.0978\n",
            "Epoch 4, Batch 1100, Loss: 0.1098\n",
            "Epoch 4, Batch 1200, Loss: 0.1061\n",
            "Epoch 4, Batch 1300, Loss: 0.1097\n",
            "Epoch 4, Batch 1400, Loss: 0.1174\n",
            "Epoch 4, Batch 1500, Loss: 0.0972\n",
            "Epoch 4, Batch 1600, Loss: 0.1183\n",
            "Epoch 4, Batch 1700, Loss: 0.1003\n",
            "Epoch 4, Batch 1800, Loss: 0.1061\n",
            "Epoch 5, Batch 100, Loss: 0.1005\n",
            "Epoch 5, Batch 200, Loss: 0.0955\n",
            "Epoch 5, Batch 300, Loss: 0.0975\n",
            "Epoch 5, Batch 400, Loss: 0.0947\n",
            "Epoch 5, Batch 500, Loss: 0.1099\n",
            "Epoch 5, Batch 600, Loss: 0.0977\n",
            "Epoch 5, Batch 700, Loss: 0.0880\n",
            "Epoch 5, Batch 800, Loss: 0.1005\n",
            "Epoch 5, Batch 900, Loss: 0.1013\n",
            "Epoch 5, Batch 1000, Loss: 0.0965\n",
            "Epoch 5, Batch 1100, Loss: 0.0899\n",
            "Epoch 5, Batch 1200, Loss: 0.0711\n",
            "Epoch 5, Batch 1300, Loss: 0.0858\n",
            "Epoch 5, Batch 1400, Loss: 0.0831\n",
            "Epoch 5, Batch 1500, Loss: 0.0854\n",
            "Epoch 5, Batch 1600, Loss: 0.1031\n",
            "Epoch 5, Batch 1700, Loss: 0.0974\n",
            "Epoch 5, Batch 1800, Loss: 0.1151\n",
            "Finished training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ],
      "metadata": {
        "id": "e6wNjtgLRSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "#Disables gradient calculation during evaluation/testing, saving memory and computation since gradients are only needed during training.\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in testloader:\n",
        "    outputs = model(inputs)\n",
        "    '''outputs.data: Contains the model's raw predictions (logits or probabilities) for each class.\n",
        "torch.max(outputs.data, 1): Identifies the class with the highest value (confidence) along dimension 1 (class axis).\n",
        " #Finds the predicted class for each input in the batch. The placeholder _ is used to indicate that we are intentionally ignoring the actual\n",
        " maximum values returned by torch.max. This simplifies the code and makes it clear that only the indices (predicted classes) are needed for further processing\n",
        " '''\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0) #labels.size(0) gives the batch size.\n",
        "    '''Compares predicted (model predictions) with labels (true targets).\n",
        "(predicted == labels) generates a boolean tensor where True indicates correct predictions.\n",
        ".sum(): Counts the number of True values (correct predictions).\n",
        ".item(): Converts the resulting tensor into a Python scalar, which is then added to correct.This code evaluates the model's performance on the test dataset\n",
        "by calculating how many predictions match the actual labels, which can then be used to compute the accuracy:\n",
        "'''\n",
        "    correct += (predicted == labels).sum().item()\n",
        "print(f\"Accuracy on the test set: {100 * correct / total}%\")"
      ],
      "metadata": {
        "id": "0s6HCs6eSm54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce8c903-5f63-4de4-bda2-1a192c7ff0c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 97.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Predictions"
      ],
      "metadata": {
        "id": "vYRh4TnglTsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some test images and the model's predictions\n",
        "#Evaluating the model gives overall metrics, while fetching and analyzing a batch like this is useful for detailed, per-sample inspection\n",
        "#This step, however, allows you to inspect individual predictions for a batch of images, which is not possible during standard evaluation.\n",
        "'''iter() converts the DataLoader into an iterator so you can manually step through its batches. It is done to To manually fetch a batch of data from the test dataset.\n",
        ".next() fetches the next batch of data (images and corresponding labels) from the iterator.\n",
        "images contains the batch of input data (test images).\n",
        "labels contains the correct labels for these test images. It is done to To get one batch of data for making predictions and evaluating the model.\n",
        "torch.max(outputs, 1) calculates the maximum value along dimension 1 (class dimension) of outputs.\n",
        "The first value (_) is the maximum value (not used here).\n",
        "The second value (predicted) is the index (class label) corresponding to the maximum value for each sample in the batch.\n",
        "To determine the predicted class labels for the test images by finding the class with the highest probability or score.\n",
        "This code manually fetches a batch of test images and labels, passes the images through the model to get predictions,\n",
        "and determines the predicted class labels by selecting the class with the highest score for each image.'''\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "outputs = model(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "#Plot the images\n",
        "#Creates a figure for plotting with a size of 10x5 inches.\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "#Iterates over the first 6 images in the batch.\n",
        "for i in range(6):\n",
        "  #Creates a grid of subplots with 2 rows and 3 columns. Places each image in the next available subplot.\n",
        "    ax = fig.add_subplot(2, 3, i + 1)\n",
        "    #Converts the ith image from a tensor to a NumPy array and displays it in grayscale.\n",
        "    ax.imshow(images[i].numpy().squeeze(), cmap='gray')\n",
        "    #Sets the title of the subplot to show the predicted label for the corresponding image.\n",
        "    ax.set_title(f\"Predicted: {predicted[i].item()}\")\n",
        "    #Removes axis labels for a cleaner look.\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Aw2wZwdOlQMt",
        "outputId": "85bc43a6-65aa-498f-bd05-4b8ed7d668d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}